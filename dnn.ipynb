{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "X0, Y = datasets.load_iris(return_X_y=True)\n",
    "Y0 = np.eye(np.max(Y)+1)[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class Gradients:\n",
    "    def __init__(self, grads):\n",
    "        self.grads = grads\n",
    "    \n",
    "    def apply(self, fn):\n",
    "        if isinstance(self.grads, dict):\n",
    "            for k, v in self.grads.items():\n",
    "                self.grads[k] = fn(k, v)\n",
    "        elif isinstance(self.grads, list):\n",
    "            self.grads = list(map(fn, self.grads))\n",
    "    \n",
    "    def apply_arg(self, op, arg):\n",
    "        if isinstance(self.grads, dict):\n",
    "            for k, v in self.grads.items():\n",
    "                self.grads[k] = op(k, v, arg[k])\n",
    "        elif isinstance(self.grads, list):\n",
    "            self.grads = [op(self.grads[i], arg[i]) for i in range(len(self.grads))]\n",
    "    \n",
    "    def applied(self, fn):\n",
    "        if isinstance(self.grads, dict):\n",
    "            new_grd = {}\n",
    "            for k, v in self.grads.items():\n",
    "                new_grd[k] = fn(k, v)\n",
    "        elif isinstance(self.grads, list):\n",
    "            new_grd = list(map(fn, self.grads))\n",
    "        return Gradients(new_grd)\n",
    "    \n",
    "    def applied_arg(self, op, arg):\n",
    "        if isinstance(self.grads, dict):\n",
    "            new_grd = {}\n",
    "            for k, v in self.grads.items():\n",
    "                new_grd[k] = op(k, v, arg[k])\n",
    "        elif isinstance(self.grads, list):\n",
    "            new_grd = [op(self.grads[i], arg[i]) for i in range(len(self.grads))]\n",
    "        return Gradients(new_grd)\n",
    "      \n",
    "    def __getitem__(self, item):\n",
    "        return self.grads[item]\n",
    "\n",
    "class AffineLayer:\n",
    "    def __init__(self, dim_in, dim_out, method='Xavier'):\n",
    "        vW = 2/(dim_in+dim_out)\n",
    "        if method == 'He':\n",
    "            vW *= 2\n",
    "        vW = sqrt(vW)\n",
    "        self.W = np.random.normal(0, vW, (dim_in, dim_out))\n",
    "        self.B = np.random.normal(0, vW, (1, dim_out))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return np.matmul(X, self.W)+self.B\n",
    "    \n",
    "    def save_forward(self, X):\n",
    "        self.pX = X\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        dW = np.matmul(self.pX.T, grad)\n",
    "        dB = np.sum(grad, axis=0, keepdims=True)\n",
    "        \n",
    "        next_grad = np.matmul(grad, self.W.T)\n",
    "        \n",
    "        return Gradients([dW, dB]), next_grad\n",
    "    \n",
    "    def update(self, u_grads):\n",
    "        self.W -= u_grads[0]\n",
    "        self.B -= u_grads[1]\n",
    "\n",
    "class DropOutlayer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "    \n",
    "    def clear_temp(self):\n",
    "        self.pFilter = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X\n",
    "    \n",
    "    def save_forward(self, X):\n",
    "        filt = np.random.binomial(1, self.rate, X.shape)\n",
    "        self.pFilter = filt\n",
    "        return filt*X\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        return self.pFilter*grad\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        self.pY = None\n",
    "        \n",
    "    def clear_temp(self):\n",
    "        self.pY = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = np.exp(X)\n",
    "        return X/(X+1)\n",
    "    \n",
    "    def save_forward(self, X):\n",
    "        Y = self.forward(X)\n",
    "        self.pY = Y\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        return self.pY*(1-self.pY)*grad\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.pY = None\n",
    "    \n",
    "    def clear_temp(self):\n",
    "        self.pY = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = np.exp(X)\n",
    "        return X/np.sum(X, axis=1, keepdims=True)\n",
    "    \n",
    "    def save_forward(self, X):\n",
    "        Y = self.forward(X)\n",
    "        self.pY = Y\n",
    "        return Y\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        Y = self.pY\n",
    "        gPY = Y[:, np.newaxis]*Y[:,:, np.newaxis]\n",
    "        PP = np.zeros_like(gPY)\n",
    "        for i in range(PP.shape[0]):\n",
    "            PP[i,:,:] = np.diag(Y[i])\n",
    "        return np.matmul(grad[:, np.newaxis], PP-gPY).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def save_forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.save_forward(X)\n",
    "        return X\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def loss(self, target, predict):\n",
    "        return -np.mean(target*np.log(predict))\n",
    "    \n",
    "    def loss_grad(self, target, predict):\n",
    "        grad = -target/predict/target.shape[0]\n",
    "        return self.loss(target, predict), grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NNOptimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.rate = learning_rate\n",
    "        self._lr_multiplier = lambda x: x*self.rate\n",
    "    \n",
    "    def loss(self, model: DNN, X, Y, loss):\n",
    "        return loss.loss(Y, model.forward(X))\n",
    "    \n",
    "    def fit(self, model: DNN, X, Y, loss):\n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        mul_lr = lambda x: x*self.rate\n",
    "        \n",
    "        for layer in reversed(model.layers):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                local_g.apply(mul_lr)\n",
    "                layer.update(local_g)\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)\n",
    "\n",
    "class SDGOptimizer(NNOptimizer):\n",
    "    def __init__(self, learning_rate, batch_size=10):\n",
    "        super().__init__(learning_rate)\n",
    "        self.bsize = batch_size\n",
    "        \n",
    "    def fit(self, model: DNN, X, Y, loss):\n",
    "        sample = np.random.choice(X.shape[0], self.bsize, replace=False)\n",
    "        X, Y = X[sample], Y[sample]\n",
    "        \n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        \n",
    "        for layer in reversed(model.layers):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                local_g.apply(self._lr_multiplier)\n",
    "                layer.update(local_g)\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)\n",
    "    \n",
    "class MomentumOptimizer(NNOptimizer):\n",
    "    def __init__(self, learning_rate, momentum=0.9):\n",
    "        super().__init__(-learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.vel = {}\n",
    "        \n",
    "    def _calc_update_gradient(self, idx, local_grad):\n",
    "        if idx not in self.vel:\n",
    "            self.vel[idx] = local_grad.applied(np.zeros_like)\n",
    "        local_grad.apply(self._lr_multiplier)\n",
    "        self.vel[idx].apply_arg(lambda prev, new: prev*self.momentum-new, local_grad)\n",
    "        return self.vel[idx]\n",
    "        \n",
    "    def fit(self, model: DNN, X, Y, loss):        \n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        \n",
    "        for idx, layer in enumerate(reversed(model.layers)):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                layer.update(self._calc_update_gradient(idx, local_g))\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)\n",
    "\n",
    "class AdaGradOptimizer(NNOptimizer):\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        super().__init__(learning_rate)\n",
    "        self.h = {}\n",
    "    \n",
    "    def _calc_update_gradient(self, idx, local_grad):\n",
    "        if idx not in self.h:\n",
    "            self.h[idx] = local_grad.applied(np.zeros_like)\n",
    "        self.h[idx].apply_arg(lambda prev, new: prev+new**2, local_grad)\n",
    "        local_grad.apply_arg(lambda x, h: x*self.rate/np.sqrt(h), self.h[idx])\n",
    "        return local_grad\n",
    "    \n",
    "    def fit(self, model: DNN, X, Y, loss):        \n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        \n",
    "        for idx, layer in enumerate(reversed(model.layers)):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                layer.update(self._calc_update_gradient(idx, local_g))\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)\n",
    "\n",
    "class RMSPropOptimizer(NNOptimizer):\n",
    "    def __init__(self, learning_rate, fp=0.9, epsi=1e-5):\n",
    "        super().__init__(learning_rate)\n",
    "        self.h = {}\n",
    "        self.fp = fp\n",
    "        self.eps = epsi\n",
    "    \n",
    "    def _calc_update_gradient(self, idx, local_grad):\n",
    "        if idx not in self.h:\n",
    "            self.h[idx] = local_grad.applied(np.zeros_like)\n",
    "        self.h[idx].apply_arg(lambda prev, new: self.fp*prev+(1-self.fp)*new**2, local_grad)\n",
    "        local_grad.apply_arg(lambda x, h: x*self.rate/(np.sqrt(h)+self.eps), self.h[idx])\n",
    "        return local_grad\n",
    "        \n",
    "    def fit(self, model: DNN, X, Y, loss):        \n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        \n",
    "        for idx, layer in enumerate(reversed(model.layers)):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                layer.update(self._calc_update_gradient(idx, local_g))\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)\n",
    "\n",
    "class AdamOptimizer(NNOptimizer):\n",
    "    def __init__(self, learning_rate, b1=0.9, b2=0.999, epsi=1e-5):\n",
    "        super().__init__(learning_rate)\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.eps = epsi\n",
    "        \n",
    "    def _to_update_grad(self, m, v):\n",
    "        M = m/(1-self.b1)\n",
    "        V = v/(1-self.b2)\n",
    "        return self.rate*M/(np.sqrt(V)+self.eps)\n",
    "        \n",
    "    def _calc_update_gradient(self, idx, local_grad):\n",
    "        if idx not in self.h:\n",
    "            self.m[idx] = local_grad.applied(np.zeros_like)\n",
    "            self.v[idx] = local_grad.applied(np.zeros_like)\n",
    "        self.m[idx].apply_arg(lambda prev, new: self.b1*prev+(1-self.b1)*new, local_grad)\n",
    "        self.v[idx].apply_arg(lambda prev, new: self.b2*prev+(1-self.b2)*new**2, local_grad)\n",
    "        \n",
    "        return self.m[idx].applied_arg(self._to_update_grad, self.v[idx])\n",
    "    \n",
    "    def fit(self, model: DNN, X, Y, loss):\n",
    "        l, grad = loss.loss_grad(Y, model.save_forward(X))\n",
    "        \n",
    "        for idx, layer in enumerate(reversed(model.layers)):\n",
    "            if hasattr(layer, 'update'):\n",
    "                local_g, grad = layer.backward(grad)\n",
    "                layer.update(self._calc_update_gradient(idx, local_g))\n",
    "            else:\n",
    "                grad = layer.backward(grad)\n",
    "            if hasattr(layer, 'clear_temp'):\n",
    "                layer.clear_temp()\n",
    "        return l, self.loss(model, X, Y, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4645503118529852"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN()\n",
    "model.add(AffineLayer(4, 5))\n",
    "model.add(SigmoidLayer())\n",
    "model.add(DropOutlayer(0.8))\n",
    "model.add(AffineLayer(5, 3))\n",
    "model.add(SoftmaxLayer())\n",
    "\n",
    "ce = CrossEntropyLoss()\n",
    "opt = RMSPropOptimizer(0.01, 0.8)\n",
    "opt.loss(model, X0, Y0, ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015345795364334886\n"
     ]
    }
   ],
   "source": [
    "for it in range(5000):\n",
    "    _, last_loss = opt.fit(model, X0, Y0, ce)\n",
    "    \n",
    "print(last_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.argmax(model.forward(X0), axis=1)==Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
